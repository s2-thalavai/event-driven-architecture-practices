# production-grade event-driven system

letâ€™s build a **small but production-grade** event-driven system with:

-   **Kafka** as the event broker
    
-   **Python** producer/consumer (Order + Payment services)
    
-   **Docker Compose** to run everything
    

## 1. Project structure

```
event-system/
â”œâ”€ docker-compose.yml
â”œâ”€ producer/          # Order Service
â”‚  â”œâ”€ Dockerfile
â”‚  â”œâ”€ requirements.txt
â”‚  â””â”€ app.py
â””â”€ consumer/          # Payment Service
   â”œâ”€ Dockerfile
   â”œâ”€ requirements.txt
   â””â”€ app.py
``` 

> Weâ€™ll use **JSON events** on a topic `orders` and a **DLQ** topic `orders.dlq`.

## 2. docker-compose.yml

``` yaml
version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"          # external
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_INTERNAL://0.0.0.0:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"  # For demo only; in real prod youâ€™d manage topics explicitly.

  order-producer:
    build: ./producer
    depends_on:
      - kafka
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      ORDERS_TOPIC: orders

  payment-consumer:
    build: ./consumer
    depends_on:
      - kafka
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      ORDERS_TOPIC: orders
      DLQ_TOPIC: orders.dlq
      GROUP_ID: payment-service

```

## 3. Producer service (Order Service)

### producer/requirements.txt

```confluent-kafka==2.5.0``` 

### producer/Dockerfile

``` dockerfile
FROM python:3.12-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app.py .

CMD ["python", "app.py"]

```

### producer/app.py

This simulates an Order Service that emits idempotent events to Kafka.

``` python
import json
import os
import signal
import sys
import time
import uuid
from datetime import datetime, timezone

from confluent_kafka import Producer

BOOTSTRAP_SERVERS = os.getenv("BOOTSTRAP_SERVERS", "kafka:29092")
ORDERS_TOPIC = os.getenv("ORDERS_TOPIC", "orders")


running = True


def handle_shutdown(signum, frame):
    global running
    print("Order Service: shutting downâ€¦")
    running = False


signal.signal(signal.SIGINT, handle_shutdown)
signal.signal(signal.SIGTERM, handle_shutdown)


def create_producer() -> Producer:
    config = {
        "bootstrap.servers": BOOTSTRAP_SERVERS,
        "client.id": "order-service",
        # Production-ish settings:
        "enable.idempotence": True,
        "acks": "all",
        "retries": 10,
        "linger.ms": 5,
        "batch.num.messages": 1000,
    }
    return Producer(config)


def delivery_report(err, msg):
    if err is not None:
        print(f"Delivery failed for record {msg.key()}: {err}")
    else:
        print(
            f"Record produced to {msg.topic()} "
            f"[partition {msg.partition()} @ offset {msg.offset()}]"
        )


def build_order_event() -> dict:
    order_id = str(uuid.uuid4())
    user_id = f"user-{uuid.uuid4().hex[:8]}"
    amount = round(10 + 90 * (uuid.uuid4().int % 100) / 100, 2)

    return {
        "event_type": "OrderPlaced",
        "event_version": 1,
        "order_id": order_id,
        "user_id": user_id,
        "amount": amount,
        "currency": "USD",
        "created_at": datetime.now(timezone.utc).isoformat(),
    }


def main():
    producer = create_producer()

    print(f"Order Service: producing to topic '{ORDERS_TOPIC}' on {BOOTSTRAP_SERVERS}")
    try:
        while running:
            event = build_order_event()
            key = event["order_id"].encode("utf-8")
            value = json.dumps(event).encode("utf-8")

            try:
                producer.produce(
                    topic=ORDERS_TOPIC,
                    key=key,
                    value=value,
                    on_delivery=delivery_report,
                )
            except BufferError as e:
                # Local buffer full, flush and retry
                print(f"Local producer queue is full ({e}), flushingâ€¦")
                producer.flush()

            # Poll to trigger delivery callbacks
            producer.poll(0)

            time.sleep(2)  # simulate incoming orders

    finally:
        print("Order Service: flushing producerâ€¦")
        producer.flush(10)
        print("Order Service: exit.")
        sys.exit(0)


if __name__ == "__main__":
    main()

```

## 4. Consumer service (Payment Service + DLQ)

### consumer/requirements.txt

```confluent-kafka==2.5.0``` 

### consumer/Dockerfile

``` dockerfile
FROM python:3.12-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app.py .

CMD ["python", "app.py"]

```

### consumer/app.py

This simulates a **Payment Service**:

-   Consumes `orders` in a **consumer group**
    
-   Processes each message
    
-   On repeated failures, sends the original event to a **DLQ topic** `orders.dlq`
    
-   Commits offsets manually after success

``` python
import json
import os
import random
import signal
import sys
import time
from datetime import datetime, timezone

from confluent_kafka import Consumer, Producer, KafkaError

BOOTSTRAP_SERVERS = os.getenv("BOOTSTRAP_SERVERS", "kafka:29092")
ORDERS_TOPIC = os.getenv("ORDERS_TOPIC", "orders")
DLQ_TOPIC = os.getenv("DLQ_TOPIC", "orders.dlq")
GROUP_ID = os.getenv("GROUP_ID", "payment-service")

MAX_PROCESSING_RETRIES = 3
running = True


def handle_shutdown(signum, frame):
    global running
    print("Payment Service: shutting downâ€¦")
    running = False


signal.signal(signal.SIGINT, handle_shutdown)
signal.signal(signal.SIGTERM, handle_shutdown)


def create_consumer() -> Consumer:
    config = {
        "bootstrap.servers": BOOTSTRAP_SERVERS,
        "group.id": GROUP_ID,
        "enable.auto.commit": False,  # manual commit
        "auto.offset.reset": "earliest",
    }
    return Consumer(config)


def create_dlq_producer() -> Producer:
    return Producer(
        {
            "bootstrap.servers": BOOTSTRAP_SERVERS,
            "client.id": "payment-service-dlq-producer",
            "acks": "all",
        }
    )


def process_payment(event: dict):
    """
    Simulate payment processing.
    In real life this would be idempotent (e.g., using order_id as key)
    and talk to an external payment gateway.
    """
    order_id = event["order_id"]
    amount = event["amount"]

    # simulate intermittent failure ~20% of the time
    if random.random() < 0.2:
        raise RuntimeError(f"Random failure processing order {order_id}")

    print(
        f"[PAYMENT] Successfully processed payment for order {order_id} "
        f"amount={amount} {event.get('currency','')}"
    )


def send_to_dlq(dlq_producer: Producer, msg, error_reason: str):
    payload = {
        "original_topic": msg.topic(),
        "original_partition": msg.partition(),
        "original_offset": msg.offset(),
        "key": msg.key().decode("utf-8") if msg.key() else None,
        "value": msg.value().decode("utf-8") if msg.value() else None,
        "error_reason": error_reason,
        "failed_at": datetime.now(timezone.utc).isoformat(),
    }

    dlq_producer.produce(
        topic=DLQ_TOPIC,
        key=payload["key"].encode("utf-8") if payload["key"] else None,
        value=json.dumps(payload).encode("utf-8"),
    )
    dlq_producer.flush(5)
    print(f"[DLQ] Sent message for key={payload['key']} to {DLQ_TOPIC}")


def main():
    consumer = create_consumer()
    dlq_producer = create_dlq_producer()

    consumer.subscribe([ORDERS_TOPIC])
    print(
        f"Payment Service: consuming from '{ORDERS_TOPIC}' as group '{GROUP_ID}'"
    )

    try:
        while running:
            msg = consumer.poll(1.0)
            if msg is None:
                continue

            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    continue
                print(f"Consumer error: {msg.error()}")
                continue

            key = msg.key().decode("utf-8") if msg.key() else None
            raw_value = msg.value().decode("utf-8")

            try:
                event = json.loads(raw_value)
            except json.JSONDecodeError as e:
                print(f"Invalid JSON, sending to DLQ: {e}")
                send_to_dlq(dlq_producer, msg, "Invalid JSON")
                consumer.commit(message=msg)
                continue

            # basic retry loop around processing
            attempts = 0
            while attempts < MAX_PROCESSING_RETRIES:
                try:
                    process_payment(event)
                    consumer.commit(message=msg)  # commit only on success
                    break
                except Exception as e:
                    attempts += 1
                    print(
                        f"[ERROR] Failed to process order_id={event.get('order_id')} "
                        f"attempt={attempts}/{MAX_PROCESSING_RETRIES}: {e}"
                    )
                    time.sleep(1)

            if attempts >= MAX_PROCESSING_RETRIES:
                print(
                    f"[ERROR] Max retries exceeded, sending order_id={event.get('order_id')} to DLQ"
                )
                send_to_dlq(dlq_producer, msg, "Max processing retries exceeded")
                consumer.commit(message=msg)

    finally:
        print("Payment Service: closing consumerâ€¦")
        consumer.close()
        print("Payment Service: exit.")
        sys.exit(0)


if __name__ == "__main__":
    main()

```
-------

## 5. Running it

From the project root:

`docker compose up --build` 

You should see:

-   `order-producer` printing delivery logs to `orders`
    
-   `payment-consumer` printing `[PAYMENT] Successfully processedâ€¦`
    
-   Occasional failures being retried, and some messages routed to `orders.dlq`


## 6. How this leans toward â€œproduction-gradeâ€

-   **Idempotent producer** (`enable.idempotence`, `acks=all`, retries)
    
-   **Consumer group** with manual commits for at-least-once semantics
    
-   **DLQ topic** for failed messages after retries
    
-   **Graceful shutdown** via `SIGINT`/`SIGTERM`
    
-   **Structured JSON events** with versioning (`event_version`)
    
-   **Config via environment variables** (good for Kubernetes later)

-----

## Add OpenTelemetry tracing + logging

1.  Add an **OTel Collector** + **Jaeger** to `docker-compose.yml`
    
2.  Instrument both **Order Service (producer)** and **Payment Service (consumer)** with:
    
    -   Traces (`TracerProvider`, spans)
        
    -   Logs (`LoggingHandler` + JSON-ish format)
        
    -   Kafka exporter via **OTLP â†’ Collector â†’ Jaeger**
        

### 1ï¸. Update docker-compose.yml

Add otel-collector and jaeger services, and pass OTEL configs to apps.

``` yaml
version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_INTERNAL://0.0.0.0:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.110.0
    command: ["--config=/etc/otel-collector-config.yml"]
    volumes:
      - ./otel-collector-config.yml:/etc/otel-collector-config.yml
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP

  # Jaeger for tracing UI
  jaeger:
    image: jaegertracing/all-in-one:1.63
    depends_on:
      - otel-collector
    environment:
      COLLECTOR_OTLP_ENABLED: "true"
    ports:
      - "16686:16686"     # UI

  order-producer:
    build: ./producer
    depends_on:
      - kafka
      - otel-collector
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      ORDERS_TOPIC: orders
      OTEL_SERVICE_NAME: order-service
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_EXPORTER_OTLP_PROTOCOL: grpc

  payment-consumer:
    build: ./consumer
    depends_on:
      - kafka
      - otel-collector
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      ORDERS_TOPIC: orders
      DLQ_TOPIC: orders.dlq
      GROUP_ID: payment-service
      OTEL_SERVICE_NAME: payment-service
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_EXPORTER_OTLP_PROTOCOL: grpc

```

### 2ï¸. Add otel-collector-config.yml (root folder)

``` yaml

receivers:
  otlp:
    protocols:
      grpc:
      http:

exporters:
  jaeger:
    endpoint: jaeger:14250
    tls:
      insecure: true

processors:
  batch:

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [jaeger]

```

> Now traces from both services (via OTLP) will show up in Jaeger at
> http://localhost:16686

## 3ï¸. Instrument the Order Service (producer)

producer/requirements.txt (extended)

``` text
confluent-kafka==2.5.0
opentelemetry-sdk==1.27.0
opentelemetry-api==1.27.0
opentelemetry-exporter-otlp==1.27.0
```

### producer/app.py (full, with tracing + logging)

``` python

import json
import logging
import os
import signal
import sys
import time
import uuid
from datetime import datetime, timezone

from confluent_kafka import Producer

from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

BOOTSTRAP_SERVERS = os.getenv("BOOTSTRAP_SERVERS", "kafka:29092")
ORDERS_TOPIC = os.getenv("ORDERS_TOPIC", "orders")

OTEL_SERVICE_NAME = os.getenv("OTEL_SERVICE_NAME", "order-service")
OTEL_EXPORTER_OTLP_ENDPOINT = os.getenv(
    "OTEL_EXPORTER_OTLP_ENDPOINT", "http://otel-collector:4317"
)

running = True

# ---------- Logging setup ----------

logger = logging.getLogger("order-service")
logger.setLevel(logging.INFO)

handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(
    logging.Formatter(
        fmt="%(asctime)s | %(name)s | %(levelname)s | %(message)s",
        datefmt="%Y-%m-%dT%H:%M:%S%z",
    )
)
logger.addHandler(handler)

# ---------- OpenTelemetry setup ----------

resource = Resource.create({"service.name": OTEL_SERVICE_NAME})

trace_provider = TracerProvider(resource=resource)
span_exporter = OTLPSpanExporter(endpoint=OTEL_EXPORTER_OTLP_ENDPOINT, insecure=True)
span_processor = BatchSpanProcessor(span_exporter)
trace_provider.add_span_processor(span_processor)
trace.set_tracer_provider(trace_provider)

tracer = trace.get_tracer(__name__)


def handle_shutdown(signum, frame):
    global running
    logger.info("Order Service: shutting downâ€¦")
    running = False


signal.signal(signal.SIGINT, handle_shutdown)
signal.signal(signal.SIGTERM, handle_shutdown)


def create_producer() -> Producer:
    config = {
        "bootstrap.servers": BOOTSTRAP_SERVERS,
        "client.id": "order-service",
        "enable.idempotence": True,
        "acks": "all",
        "retries": 10,
        "linger.ms": 5,
        "batch.num.messages": 1000,
    }
    return Producer(config)


def delivery_report(err, msg):
    if err is not None:
        logger.error(
            "Delivery failed",
            extra={"key": msg.key(), "topic": msg.topic(), "error": str(err)},
        )
    else:
        logger.info(
            "Record produced",
            extra={
                "topic": msg.topic(),
                "partition": msg.partition(),
                "offset": msg.offset(),
            },
        )


def build_order_event() -> dict:
    order_id = str(uuid.uuid4())
    user_id = f"user-{uuid.uuid4().hex[:8]}"
    amount = round(10 + 90 * (uuid.uuid4().int % 100) / 100, 2)

    return {
        "event_type": "OrderPlaced",
        "event_version": 1,
        "order_id": order_id,
        "user_id": user_id,
        "amount": amount,
        "currency": "USD",
        "created_at": datetime.now(timezone.utc).isoformat(),
    }


def main():
    producer = create_producer()

    logger.info(
        "Order Service started",
        extra={"topic": ORDERS_TOPIC, "bootstrap_servers": BOOTSTRAP_SERVERS},
    )

    try:
        while running:
            event = build_order_event()

            # Create a span for publishing the order
            with tracer.start_as_current_span("publish_order") as span:
                span.set_attribute("messaging.system", "kafka")
                span.set_attribute("messaging.destination", ORDERS_TOPIC)
                span.set_attribute("order.id", event["order_id"])
                span.set_attribute("user.id", event["user_id"])
                span.set_attribute("amount", event["amount"])

                key = event["order_id"].encode("utf-8")
                value = json.dumps(event).encode("utf-8")

                try:
                    producer.produce(
                        topic=ORDERS_TOPIC,
                        key=key,
                        value=value,
                        on_delivery=delivery_report,
                    )
                    logger.info(
                        "Order event produced",
                        extra={
                            "order_id": event["order_id"],
                            "user_id": event["user_id"],
                            "amount": event["amount"],
                        },
                    )
                except BufferError as e:
                    logger.warning(
                        "Local producer queue is full; flushing",
                        extra={"error": str(e)},
                    )
                    producer.flush()

                producer.poll(0)

            time.sleep(2)

    finally:
        logger.info("Flushing producerâ€¦")
        producer.flush(10)
        logger.info("Order Service exit.")
        trace_provider.shutdown()
        sys.exit(0)


if __name__ == "__main__":
    main()

```


## 4ï¸. Instrument the Payment Service (consumer)

### consumer/requirements.txt

```
confluent-kafka==2.5.0
opentelemetry-sdk==1.27.0
opentelemetry-api==1.27.0
opentelemetry-exporter-otlp==1.27.0
``` 

### consumer/app.py (full, with tracing + logging)

``` python
import json
import logging
import os
import random
import signal
import sys
import time
from datetime import datetime, timezone

from confluent_kafka import Consumer, Producer, KafkaError

from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

BOOTSTRAP_SERVERS = os.getenv("BOOTSTRAP_SERVERS", "kafka:29092")
ORDERS_TOPIC = os.getenv("ORDERS_TOPIC", "orders")
DLQ_TOPIC = os.getenv("DLQ_TOPIC", "orders.dlq")
GROUP_ID = os.getenv("GROUP_ID", "payment-service")

OTEL_SERVICE_NAME = os.getenv("OTEL_SERVICE_NAME", "payment-service")
OTEL_EXPORTER_OTLP_ENDPOINT = os.getenv(
    "OTEL_EXPORTER_OTLP_ENDPOINT", "http://otel-collector:4317"
)

MAX_PROCESSING_RETRIES = 3
running = True

# ---------- Logging ----------

logger = logging.getLogger("payment-service")
logger.setLevel(logging.INFO)

handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(
    logging.Formatter(
        fmt="%(asctime)s | %(name)s | %(levelname)s | %(message)s",
        datefmt="%Y-%m-%dT%H:%M:%S%z",
    )
)
logger.addHandler(handler)

# ---------- OpenTelemetry ----------

resource = Resource.create({"service.name": OTEL_SERVICE_NAME})

trace_provider = TracerProvider(resource=resource)
span_exporter = OTLPSpanExporter(endpoint=OTEL_EXPORTER_OTLP_ENDPOINT, insecure=True)
span_processor = BatchSpanProcessor(span_exporter)
trace_provider.add_span_processor(span_processor)
trace.set_tracer_provider(trace_provider)

tracer = trace.get_tracer(__name__)


def handle_shutdown(signum, frame):
    global running
    logger.info("Payment Service: shutting downâ€¦")
    running = False


signal.signal(signal.SIGINT, handle_shutdown)
signal.signal(signal.SIGTERM, handle_shutdown)


def create_consumer() -> Consumer:
    config = {
        "bootstrap.servers": BOOTSTRAP_SERVERS,
        "group.id": GROUP_ID,
        "enable.auto.commit": False,
        "auto.offset.reset": "earliest",
    }
    return Consumer(config)


def create_dlq_producer() -> Producer:
    return Producer(
        {
            "bootstrap.servers": BOOTSTRAP_SERVERS,
            "client.id": "payment-service-dlq-producer",
            "acks": "all",
        }
    )


def process_payment(event: dict):
    order_id = event["order_id"]
    amount = event["amount"]

    # Simulate 20% random failure
    if random.random() < 0.2:
        raise RuntimeError(f"Random failure processing order {order_id}")

    logger.info(
        "[PAYMENT] Payment processed",
        extra={"order_id": order_id, "amount": amount, "currency": event.get("currency")},
    )


def send_to_dlq(dlq_producer: Producer, msg, error_reason: str):
    payload = {
        "original_topic": msg.topic(),
        "original_partition": msg.partition(),
        "original_offset": msg.offset(),
        "key": msg.key().decode("utf-8") if msg.key() else None,
        "value": msg.value().decode("utf-8") if msg.value() else None,
        "error_reason": error_reason,
        "failed_at": datetime.now(timezone.utc).isoformat(),
    }

    dlq_producer.produce(
        topic=DLQ_TOPIC,
        key=payload["key"].encode("utf-8") if payload["key"] else None,
        value=json.dumps(payload).encode("utf-8"),
    )
    dlq_producer.flush(5)
    logger.warning(
        "[DLQ] Message sent to DLQ",
        extra={"dlq_topic": DLQ_TOPIC, "key": payload["key"], "reason": error_reason},
    )


def main():
    consumer = create_consumer()
    dlq_producer = create_dlq_producer()

    consumer.subscribe([ORDERS_TOPIC])
    logger.info(
        "Payment Service started",
        extra={
            "orders_topic": ORDERS_TOPIC,
            "group_id": GROUP_ID,
            "bootstrap_servers": BOOTSTRAP_SERVERS,
        },
    )

    try:
        while running:
            msg = consumer.poll(1.0)
            if msg is None:
                continue

            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    continue
                logger.error("Consumer error", extra={"error": str(msg.error())})
                continue

            key = msg.key().decode("utf-8") if msg.key() else None
            raw_value = msg.value().decode("utf-8")

            # span around "consume + process"
            with tracer.start_as_current_span("consume_order") as span:
                span.set_attribute("messaging.system", "kafka")
                span.set_attribute("messaging.destination", ORDERS_TOPIC)
                span.set_attribute("messaging.kafka.partition", msg.partition())
                span.set_attribute("messaging.kafka.offset", msg.offset())
                if key:
                    span.set_attribute("messaging.kafka.key", key)

                try:
                    event = json.loads(raw_value)
                except json.JSONDecodeError as e:
                    logger.error(
                        "Invalid JSON, sending to DLQ",
                        extra={"error": str(e), "raw": raw_value},
                    )
                    send_to_dlq(dlq_producer, msg, "Invalid JSON")
                    consumer.commit(message=msg)
                    continue

                # add business attributes to span
                span.set_attribute("order.id", event.get("order_id"))
                span.set_attribute("user.id", event.get("user_id"))
                span.set_attribute("amount", event.get("amount"))

                attempts = 0
                while attempts < MAX_PROCESSING_RETRIES:
                    try:
                        process_payment(event)
                        consumer.commit(message=msg)
                        break
                    except Exception as e:
                        attempts += 1
                        logger.error(
                            "Failed to process payment",
                            extra={
                                "order_id": event.get("order_id"),
                                "attempt": attempts,
                                "max_attempts": MAX_PROCESSING_RETRIES,
                                "error": str(e),
                            },
                        )
                        span.record_exception(e)
                        span.set_attribute("retry.attempt", attempts)
                        time.sleep(1)

                if attempts >= MAX_PROCESSING_RETRIES:
                    logger.error(
                        "Max retries exceeded, sending to DLQ",
                        extra={"order_id": event.get("order_id")},
                    )
                    send_to_dlq(
                        dlq_producer,
                        msg,
                        "Max processing retries exceeded",
                    )
                    consumer.commit(message=msg)

    finally:
        logger.info("Payment Service: closing consumerâ€¦")
        consumer.close()
        trace_provider.shutdown()
        logger.info("Payment Service exit.")
        sys.exit(0)


if __name__ == "__main__":
    main()
```


## 5ï¸. Running and viewing traces

From the project root:

`docker compose up --build` 

Then open **Jaeger UI** in your browser:

-   URL: `http://localhost:16686`
    
-   Choose service: `order-service` or `payment-service`
    
-   Youâ€™ll see spans like `publish_order` and `consume_order` with attributes:
    
    -   `messaging.system = kafka`
        
    -   `messaging.destination = orders`
        
    -   `order.id`, `user.id`, `amount`, etc.
        

Logs will still show up in the container output (`docker compose logs -f`).

------

## Add correlation between services using W3C trace context in Kafka message headers


**Kafka-based event-driven system** so that **traces correlate across microservices** (Order â†’ Payment) using **W3C Trace Context** headers.

This enables full **distributed tracing** in Jaeger â€” youâ€™ll see the _Order Service_â€™s `publish_order` span as the **parent** of the _Payment Service_â€™s `consume_order` span.

## What Weâ€™ll Do

| Step | Description                                                                                 |
| ---- | ------------------------------------------------------------------------------------------- |
| 1ï¸  | Inject W3C trace context (trace ID, span ID) into Kafka message headers in the **producer** |
| 2ï¸  | Extract that context from Kafka headers in the **consumer**                                 |
| 3ï¸  | Continue the trace (propagation) so both services share the same trace in Jaeger            |


Weâ€™ll use OpenTelemetryâ€™s **propagators**:

-   `TraceContextTextMapPropagator` (W3C standard)
    
-   `inject()` / `extract()` methods to pass trace context via Kafka headers


## 1ï¸. Update Order Service (Producer)

In `producer/app.py`, modify the message publish logic to **inject** the trace context into Kafka headers.

Add this import:

```python
from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator
``` 

Then modify the part where we publish the event:

```python
from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator

propagator = TraceContextTextMapPropagator()
``` 

Inside your loop, replace the inner `with tracer.start_as_current_span("publish_order"):` block with this version:

``` python
with tracer.start_as_current_span("publish_order") as span:
    span.set_attribute("messaging.system", "kafka")
    span.set_attribute("messaging.destination", ORDERS_TOPIC)
    span.set_attribute("order.id", event["order_id"])
    span.set_attribute("user.id", event["user_id"])
    span.set_attribute("amount", event["amount"])

    # --- Inject W3C Trace Context into Kafka headers ---
    carrier = {}
    propagator.inject(carrier)
    kafka_headers = [(k, v.encode("utf-8")) for k, v in carrier.items()]

    key = event["order_id"].encode("utf-8")
    value = json.dumps(event).encode("utf-8")

    try:
        producer.produce(
            topic=ORDERS_TOPIC,
            key=key,
            value=value,
            headers=kafka_headers,  # inject trace headers
            on_delivery=delivery_report,
        )
        logger.info(
            "Order event produced",
            extra={
                "order_id": event["order_id"],
                "trace_id": span.get_span_context().trace_id,
            },
        )
    except BufferError as e:
        logger.warning("Local producer queue full; flushing", extra={"error": str(e)})
        producer.flush()

```


Now every Kafka message has headers like:

```ini
traceparent=00-<trace_id>-<span_id>-01
``` 

These follow the [W3C Trace Context standard](https://www.w3.org/TR/trace-context/).


## 2ï¸. Update Payment Service (Consumer)

In `consumer/app.py`, weâ€™ll **extract** that context from Kafka headers and continue the trace.

Add this import near the top:

``` python
from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator
``` 

Then, before the `with tracer.start_as_current_span("consume_order")` block, extract the context:

Replace this section inside your loop:

``` python
# span around "consume + process"
with tracer.start_as_current_span("consume_order") as span:
``` 

with this:

``` python

propagator = TraceContextTextMapPropagator()

# Extract trace context from Kafka headers
carrier = {}
if msg.headers():
    for k, v in msg.headers():
        if isinstance(v, bytes):
            carrier[k] = v.decode("utf-8")
        else:
            carrier[k] = v

context = propagator.extract(carrier)

# Continue trace context
with tracer.start_as_current_span("consume_order", context=context) as span:

```

> This ensures that if `Order Service` publishes an event, the `Payment Service` span will share the same trace ID.

## 3ï¸. Optional: Log Trace IDs

To make debugging easier, include trace and span IDs in logs.

Add this utility:

``` python

def get_trace_context():
    span = trace.get_current_span()
    ctx = span.get_span_context()
    if ctx.trace_id:
        return {
            "trace_id": f"{ctx.trace_id:032x}",
            "span_id": f"{ctx.span_id:016x}",
        }
    return {}
```

Then use it in logs, e.g.:

``` python
logger.info(
    "Processing payment",
    extra={**get_trace_context(), "order_id": event["order_id"]},
)

```


## 4ï¸. Run It All

`docker compose up --build` 

Then open **Jaeger UI**:

ğŸ”— `http://localhost:16686`

### Youâ€™ll see:

-   A trace with both:
    
    -   `order-service` â†’ `publish_order`
        
    -   `payment-service` â†’ `consume_order`
        
-   The same `trace_id` across both spans
    

----------

## 5ï¸. Example Trace Flow

``` css
Order Service (Span A)
  â””â”€â”€ publish_order
       â†³ inject trace headers into Kafka message
            â†“
Payment Service (Span B)
  â””â”€â”€ consume_order (child span)
       â†³ extract headers â†’ continue same trace_id
```

So the trace graph in Jaeger shows one end-to-end flow:

``` css
Order Service â†’ Kafka â†’ Payment Service
```

## Result: Full Distributed Observability

| Capability      | Achieved by                                    |
| --------------- | ---------------------------------------------- |
| **Tracing**     | OpenTelemetry + Jaeger                         |
| **Metrics**     | (can add Prometheus exporter next)             |
| **Logging**     | Structured JSON logs with trace IDs            |
| **Correlation** | W3C Trace Context propagation in Kafka headers |
| **Resilience**  | Retries + DLQ                                  |
| **Scalability** | Kafka consumer groups                          |

-----


## **production-grade observability stack**: 

**Traces (Jaeger) + Metrics (Prometheus) + Logs (stdout / ELK)**.

Letâ€™s integrate **OpenTelemetry Metrics** into your existing system:

-   Both microservices (`order-service`, `payment-service`) will expose metrics via **OTel SDK**
    
-   Metrics â†’ **OpenTelemetry Collector â†’ Prometheus â†’ Grafana**


## Overview

Hereâ€™s the data flow after adding metrics:

``` css
[Order Service]      [Payment Service]
      â”‚                     â”‚
      â””â”€â”€ OpenTelemetry Metrics Exporter â”€â”€â–¶ OTel Collector â”€â”€â–¶ Prometheus â”€â”€â–¶ Grafana
```

Weâ€™ll record:

-   Event production count, latency, and failures
    
-   Message consumption count and processing latency
    
-   DLQ events
    
-   System health (exporter heartbeat)

## 1ï¸. Extend docker-compose.yml

Add Prometheus and Grafana, and configure OTel Collector to export metrics to Prometheus.

``` yaml

version: "3.8"

services:
  # ... existing Kafka, OTel Collector, Jaeger ...

  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  # Grafana
  grafana:
    image: grafana/grafana:latest
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning

  order-producer:
    build: ./producer
    depends_on:
      - kafka
      - otel-collector
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      ORDERS_TOPIC: orders
      OTEL_SERVICE_NAME: order-service
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_EXPORTER_OTLP_PROTOCOL: grpc
      OTEL_METRIC_INTERVAL: 5

  payment-consumer:
    build: ./consumer
    depends_on:
      - kafka
      - otel-collector
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      ORDERS_TOPIC: orders
      DLQ_TOPIC: orders.dlq
      GROUP_ID: payment-service
      OTEL_SERVICE_NAME: payment-service
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      OTEL_EXPORTER_OTLP_PROTOCOL: grpc
      OTEL_METRIC_INTERVAL: 5
```

## 2ï¸. Update otel-collector-config.yml

Add a metrics pipeline (Prometheus exporter).

``` yaml
receivers:
  otlp:
    protocols:
      grpc:
      http:

exporters:
  jaeger:
    endpoint: jaeger:14250
    tls:
      insecure: true
  prometheus:
    endpoint: "0.0.0.0:9464"  # where Prometheus will scrape metrics

processors:
  batch:

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [jaeger]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [prometheus]

```

## 3ï¸. Add prometheus.yml

``` yaml
global:
  scrape_interval: 10s

scrape_configs:
  - job_name: "otel-collector"
    static_configs:
      - targets: ["otel-collector:9464"]

```

> Prometheus scrapes the OTel Collector metrics endpoint every 10s.

## 4ï¸. Add Grafana datasource & dashboard provisioning (optional)
grafana/provisioning/datasources/datasource.yml

```yaml
apiVersion: 1
datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
```

You can add dashboards manually via the UI at http://localhost:3000
(Login: admin / admin)

## 5ï¸. Update Python Services to Record Metrics

Both services will use:

``` bash
  pip install opentelemetry-sdk opentelemetry-api opentelemetry-exporter-otlp
```

### Add to producer/app.py

Below your tracing setup, add metric instruments.

``` python
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.metrics import get_meter
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter

# Metric setup
metric_exporter = OTLPMetricExporter(endpoint=OTEL_EXPORTER_OTLP_ENDPOINT, insecure=True)
reader = PeriodicExportingMetricReader(metric_exporter)
meter_provider = MeterProvider(resource=resource, metric_readers=[reader])
meter = get_meter(__name__)

# Instruments
orders_counter = meter.create_counter(
    "orders_published_total",
    unit="1",
    description="Number of order events produced",
)
orders_failed = meter.create_counter(
    "orders_failed_total",
    unit="1",
    description="Number of failed order events",
)
publish_duration = meter.create_histogram(
    "order_publish_latency_ms",
    unit="ms",
    description="Latency of producing an order event",
)

```

Then wrap your producer.produce(...) inside timing logic:

``` python
import time

start_time = time.perf_counter()
try:
    producer.produce(topic=ORDERS_TOPIC, key=key, value=value, headers=kafka_headers)
    duration = (time.perf_counter() - start_time) * 1000
    publish_duration.record(duration)
    orders_counter.add(1)
except BufferError as e:
    orders_failed.add(1)
    logger.warning("Queue full; flushing", extra={"error": str(e)})
    producer.flush()

```

### Add to consumer/app.py

Similar setup for consumption metrics:

``` python
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.metrics import get_meter
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter

metric_exporter = OTLPMetricExporter(endpoint=OTEL_EXPORTER_OTLP_ENDPOINT, insecure=True)
reader = PeriodicExportingMetricReader(metric_exporter)
meter_provider = MeterProvider(resource=resource, metric_readers=[reader])
meter = get_meter(__name__)

# Instruments
messages_consumed = meter.create_counter(
    "messages_consumed_total",
    unit="1",
    description="Number of order events consumed",
)
processing_latency = meter.create_histogram(
    "payment_processing_latency_ms",
    unit="ms",
    description="Latency of payment processing",
)
failed_messages = meter.create_counter(
    "messages_failed_total",
    unit="1",
    description="Number of payment processing failures",
)
dlq_messages = meter.create_counter(
    "dlq_messages_total",
    unit="1",
    description="Number of messages sent to DLQ",
)
```

### Then update your consumer loop:

``` python
start_time = time.perf_counter()
try:
    process_payment(event)
    duration = (time.perf_counter() - start_time) * 1000
    processing_latency.record(duration)
    messages_consumed.add(1)
    consumer.commit(message=msg)
except Exception as e:
    failed_messages.add(1)
    # existing retry logic...
```

And when you send to DLQ:

``` python
dlq_messages.add(1)
```

## 6ï¸. Run Everything

```
docker compose up --build
```

-   **Jaeger UI:** http://localhost:16686
    
-   **Prometheus UI:** http://localhost:9090
    
-   **Grafana UI:** http://localhost:3000

## 7ï¸. Example PromQL Metrics (in Prometheus or Grafana)

| Metric                          | Description                     | Query                                                                              |
| ------------------------------- | ------------------------------- | ---------------------------------------------------------------------------------- |
| `orders_published_total`        | Count of order events produced  | `sum(orders_published_total)`                                                      |
| `order_publish_latency_ms`      | Publish latency histogram       | `histogram_quantile(0.95, sum(rate(order_publish_latency_ms_bucket[5m])) by (le))` |
| `messages_consumed_total`       | Count of processed order events | `rate(messages_consumed_total[1m])`                                                |
| `payment_processing_latency_ms` | Processing latency histogram    | same as above                                                                      |
| `messages_failed_total`         | Total failed events             | `sum(messages_failed_total)`                                                       |
| `dlq_messages_total`            | Total messages sent to DLQ      | `sum(dlq_messages_total)`                                                          |

## Final Architecture (Production-Grade)

```
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚      Order Service (Python)  â”‚
                   â”‚   â”€â”€ Traces + Metrics + Logs â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                             (Kafka)
                                 â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚    Payment Service (Python)  â”‚
                   â”‚   â”€â”€ Traces + Metrics + Logs â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ OTel Collector â”‚
                         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                         â”‚  â†’ Jaeger (traces)
                         â”‚  â†’ Prometheus (metrics)
                         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Grafana UI    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```
